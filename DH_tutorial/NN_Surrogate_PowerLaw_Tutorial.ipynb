{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34e4e66",
   "metadata": {},
   "source": [
    "# Practical Machine Learning for X-ray Astronomy: Building a Neural-Network Surrogate for a Power-Law Spectrum\n",
    "\n",
    "This tutorial is a hands-on introduction to practical machine learning for X-ray astronomers. \n",
    "\n",
    "**Note**: it should run entirely or almost entirely on **Google Colab**, so if for any reason you can't install the libraries, upload it to Colab and try running it there!\n",
    "\n",
    "As a note in advance: none of the results you'll get out of this are science-worthy. This tutorial is meant to give you a first idea for how to set up your own machine learning model. But the first, and most important lesson, is this: **don't blindly trust your ML results.** \n",
    "\n",
    "As with any other science project, reporting or using results from a machine learning classifier or regressor requires careful understanding of the biases and caveats, assumptions and limitations that come with the data and algorithms chosen. With building surrogate models especially, you can expect to find all the funny weird corner cases (both subtle and not) of the model you're trying to emulate. In a real-world setting, you need to really understand the limitations of the model you're using, before drawing any scientific conclusions from your machine learning.\n",
    "\n",
    "With that out of the way, let's have some fun with machine learning! In this tutorial, we will use python, a library called `scikit-learn` and `pytorch` to do our machine learning, `pandas` to deal with data structures, and `matplotlib` and `seaborn` to do our plotting. \n",
    "\n",
    "You will build a **neural-network surrogate model** for a simple X-ray spectral model: a **power law**. This is obviously a silly endeavour: a power law is a simple model you can just write down analytically. But here, it serves as a simple (fast) analogue to much more complex numerical models you can't write down analytically, and who might be expensive to compute. Our goal is to build a neural network replacement for this model, which accepts sets of parameters and produces the X-ray spectrum of the given physical model, ideally orders of magnitude faster than the original physics model. The basic idea is this: if we need to evaluate this model *a lot* (hundreds of thousands or millions of times, e.g. for inferring posterior distributions of parameters via Markov Chain Monte Carlo sampling), then doing this once in order to generate a training data set for a surrogate model might be much more computationally efficient than using the model directly for inference. \n",
    "\n",
    "The goal is not to build the best model ever, but to learn the workflow and the *failure modes* of ML in scientific settings.\n",
    "\n",
    "We will keep the focus on:\n",
    "- train/validation/test splits\n",
    "- overfitting and generalization\n",
    "- scaling/normalization\n",
    "- cross-validation and hyperparameter selection\n",
    "- robust evaluation\n",
    "\n",
    "We will start deliberately too small:\n",
    "1. Train a single-neuron network on one training example  \n",
    "2. Show it fails on validation data  \n",
    "3. Scale up the training set and the model capacity in a controlled way\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0842e",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "There are basically two major packages for building and working with neural networks: [`tensorflow`](https://www.tensorflow.org) and [`pytorch`](https://pytorch.org). They are somewhat different in syntax, but can both use all of the major things you're likely going to want to do, so it doesn't matter much which one you pick. Here, we're going to use `pytorch`. \n",
    "\n",
    "`scikit-learn` is a more general machine learning package that has a wide range of algorithms and infrastructure for preprocessing data, exploring the performance of your machine learning models and so on. So this one's useful to have in your arsenal. \n",
    "\n",
    "If you're exploring different models (e.g. different architectures, activation functions, hyperparameters, ...), [`weights and biases`](https://wandb.ai) can be a useful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629ba98-047d-4864-a10b-749a7c209995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS CELL IF YOU RUN ON COLAB\n",
    "#!pip install emcee\n",
    "#!pip install corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import gamma as scipy_gamma\n",
    "from scipy.special import gammaln as scipy_gammaln\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import qmc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "# Reproducibility (still not perfect on GPU, but good enough for this tutorial)\n",
    "rng = np.random.default_rng(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0f6e5",
   "metadata": {},
   "source": [
    "## A useful plotting function\n",
    "\n",
    "We'll be predicting an entire spectrum (counts as a function of energy).  \n",
    "The helper below makes quick “data vs prediction” plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum(energy, y_true, y_pred=None, title=None, ax=None):\n",
    "    \"\"\"Plot a spectrum in counts vs energy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energy : array, shape (nE,)\n",
    "        Energy grid in keV.\n",
    "    y_true : array, shape (nE,)\n",
    "        True target spectrum (counts).\n",
    "    y_pred : array, shape (nE,), optional\n",
    "        Predicted spectrum (counts).\n",
    "    title : str, optional\n",
    "        Plot title.\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(7,4))\n",
    "\n",
    "    ax.plot(energy, y_true, marker='o', lw=1, label=\"target\")\n",
    "    if y_pred is not None:\n",
    "        ax.plot(energy, y_pred, marker='.', markersize=3, lw=1, label=\"prediction\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Energy (keV)\")\n",
    "    ax.set_ylabel(\"Counts (arb.)\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c22de",
   "metadata": {},
   "source": [
    "## The scenario\n",
    "\n",
    "In X-ray spectral fitting we often evaluate a forward model (i.e. a physics model, usually modified to account for instrumental distortions) many times:\n",
    "- to explore a likelihood surface\n",
    "- to infer posterior distributions of model parameters via sampling\n",
    "- population inference with many spectra\n",
    "\n",
    "If each forward-model evaluation is expensive (response folding, complex physical models, etc.), a **(neural network) surrogate** can accelerate inference. You train a neural network to approximate the forward model and then use it as a fast emulator.\n",
    "\n",
    "Here we'll emulate a simple model:\n",
    "\n",
    "$$\n",
    "N(E) = A\\,(E/E_0)^{-\\Gamma}\n",
    "$$\n",
    "\n",
    "As mentioned above, this is a bit of silly exercise: a power law is an analytic function that's really fast to evaluate. However, for more expensive numerical models, this might be a way to significantly speed up evaluation time. We're using the simple model here to give you something reasonable to work with as part of the tutorial, and so that you don't have to spend ages waiting for your training data to generate.\n",
    "\n",
    "We're also not going to worry about instrument responses here. The idea of the surrogate model is that you use it as a **drop-in replacement** for the physical model, and then apply the instrument responses to the output as you would for your physical model during inference. You *could* also learn the model convolved with the response (and that may be efficient when the response calculations are suddenly the most expensive part of the calculation, see also: XRISM), but be aware that this would tie your trained surrogate model very strongly to a single instrument, and you'd have to re-train for any new instrument you want to use it for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff79264",
   "metadata": {},
   "source": [
    "### Generate synthetic spectra\n",
    "\n",
    "In order to train a neural network to emulate the physical model above, we need to pre-generate a **training data set**. This is a data set that contains matched pairs of **parameters** ($A$ and $\\Gamma$) and the corresponding model output fluxes at a set of photon energies. How big that data set should be is hard to estimate in advance: it depends on the complexity of the model and the number of parameters. \n",
    "\n",
    "We choose:\n",
    "- energy grid: 0.3–10 keV\n",
    "- parameters: amplitude $A$ and photon index $\\Gamma$\n",
    "\n",
    "We will *learn* the mapping:\n",
    "\n",
    "$$\n",
    "(A, \\Gamma) \\rightarrow \\{\\mathrm{counts}(E_i)\\}_{i=1}^{n_E}\n",
    "$$\n",
    "\n",
    "This is a **multi-output regression** problem. Here, we're going to simulate data and build a surrogate model for a **fixed energy grid**. However, there have recently been types of neural networks introduced that are independent of the input grid (e.g. Neural Operators, DeepONets, ...).\n",
    "\n",
    "**Note**: generally, we don't want to train a neural network on the amplitude parameter here. This is because for many models, the amplitude parameter just moves the spectrum up and down in flux space, which is a really cheap mathematical operation we can just apply ourselves after calculating the spectrum, and thus the neural network doesn't have to learn it. I'm leaving it in here explicitly specifically so that we have more than one parameter to train on, and also so you can explore training on parameters that span multiple orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw_counts(energy, amp, gamma, e0=1.0, exposure=1.0):\n",
    "    \"\"\"\n",
    "    Deterministic toy counts spectrum (no noise).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    energy : array, shape (nE,)\n",
    "        Energy grid in keV.\n",
    "\n",
    "    amp : float\n",
    "        The power law amplitude\n",
    "\n",
    "    gamma: float\n",
    "        The power law index\n",
    "\n",
    "    e0: float\n",
    "        The break energy?\n",
    "\n",
    "    exposure : float\n",
    "        The exposure time of the observation\n",
    "    \"\"\"\n",
    "    photon_flux = amp * (energy/e0)**(-gamma)  # photons / (keV * cm^2 * s) up to a constant\n",
    "    return photon_flux * exposure\n",
    "\n",
    "def simulate_dataset_random(n_samples, energy, \n",
    "                          logamp_range=(np.log(5e-4), np.log(5e-2)), \n",
    "                          gamma_range=(1.0, 3.0), \n",
    "                          exposure=1.0, noisy=False):\n",
    "    \"\"\"\n",
    "    Simulate a dataset of (parameters -> spectrum), \n",
    "    using simple uniform random sampling (in log-space for the amplitude).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        The number of samples to draw in each dimension. Full dataset will be \n",
    "        of size nsamples\n",
    "\n",
    "    energy : array\n",
    "        The energy grid to simulate the data over\n",
    "\n",
    "    logamp_range : (float, float)\n",
    "        The lower and upper boundary between which to sample \n",
    "        the amplitude\n",
    "\n",
    "    gamma_range : (float, float)\n",
    "        The lower and upper boundary between which to sample \n",
    "        the power law index\n",
    "\n",
    "    exposure : float\n",
    "        The exposure of the observation\n",
    "\n",
    "    noisy : bool, default False\n",
    "        If True, add Poisson noise to the spectrum\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array of shape (nsamples, 2)\n",
    "        The array containing all the pairs of parameters\n",
    "\n",
    "    Y : array of (nsamples, len(energy))\n",
    "        The array with all simulated spectra\n",
    "    \"\"\"\n",
    "    logamp = rng.uniform(logamp_range[0], logamp_range[1], size=n_samples)\n",
    "    amp = np.exp(logamp)\n",
    "    gamma = rng.uniform(gamma_range[0], gamma_range[1], size=n_samples)\n",
    "\n",
    "    X = np.column_stack([logamp, gamma]).astype(np.float32)  # use logA for dynamic range\n",
    "    Y = np.stack([powerlaw_counts(energy, ampi, gammai, exposure=exposure) for ampi, gammai in zip(amp, gamma)]).astype(np.float32)\n",
    "\n",
    "    if noisy:\n",
    "        # Poisson noise around the expected counts\n",
    "        Y = rng.poisson(Y).astype(np.float32)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def simulate_dataset_lhc(n_samples, energy, \n",
    "                          logamp_range=(np.log(5e-4), np.log(5e-2)), \n",
    "                          gamma_range=(1.0, 3.0), \n",
    "                          exposure=1.0, noisy=False):\n",
    "    \"\"\"\n",
    "    Simulate a dataset of (parameters -> spectrum), \n",
    "    using a Latin Hypercube Sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        The number of samples to draw in each dimension. Full dataset will be \n",
    "        of size nsamples\n",
    "\n",
    "    energy : array\n",
    "        The energy grid to simulate the data over\n",
    "\n",
    "    logamp_range : (float, float)\n",
    "        The lower and upper boundary between which to sample \n",
    "        the amplitude\n",
    "\n",
    "    gamma_range : (float, float)\n",
    "        The lower and upper boundary between which to sample \n",
    "        the power law index\n",
    "\n",
    "    exposure : float\n",
    "        The exposure of the observation\n",
    "\n",
    "    noisy : bool, default False\n",
    "        If True, add Poisson noise to the spectrum\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array of shape (nsamples, 2)\n",
    "        The array containing all the pairs of parameters\n",
    "\n",
    "    Y : array of (nsamples, len(energy))\n",
    "        The array with all simulated spectra \n",
    "    \"\"\"\n",
    "    # set up Latin Hypercube Sampling\n",
    "    sampler = qmc.LatinHypercube(d=2)\n",
    "\n",
    "    # sample randomly using LHS\n",
    "    sample = sampler.random(n=n_samples)\n",
    "\n",
    "    # set array of lower and upper bounds\n",
    "    l_bounds = [logamp_range[0], gamma_range[0]]\n",
    "    u_bounds = [logamp_range[1], gamma_range[1]]\n",
    "\n",
    "    # scale sample to bounds\n",
    "    X = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "    Y = np.stack([powerlaw_counts(energy, np.exp(log_ampi), gammai, exposure=exposure) for log_ampi, gammai in X]).astype(np.float32)\n",
    "\n",
    "    if noisy:\n",
    "        # Poisson noise around the expected counts\n",
    "        Y = rng.poisson(Y).astype(np.float32)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f8ff3-b893-483a-8118-c561d2e5725f",
   "metadata": {},
   "source": [
    "Above, there are two sampling functions. One uses uniform random sampling, the other uses [Latin Hypercube Sampling](https://en.wikipedia.org/wiki/Latin_hypercube_sampling). Both tend to do better than a uniform, regularly spaced grid, and LHS tends to do better than random uniform sampling in higher dimensions. LHS is designed to evenly fill the high-dimensional space with points, whereas uniform random sampling can produce clumps in high dimensions. \n",
    "\n",
    "Grids tend to be very sparse: if you have a 10-dimensional problem and just want 5 points in each dimension, you already need nearly a million grid points total! Neural networks tend to overfit on the grid points, and do much worse in between, and some of that can be mitigated by some form of random sampling.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948031e9-bdd1-4d8e-86c7-80e54f1dd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate energy grid\n",
    "energy = np.logspace(np.log10(0.3), np.log10(10.0), 60).astype(np.float32)\n",
    "\n",
    "# make some simulated data \n",
    "X_demo, Y_demo = simulate_dataset_random(100, energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce170f1-746b-4b6f-8235-4a136d9a9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax1.scatter(X_demo[:,0], X_demo[:,1], s=2)\n",
    "\n",
    "for i in range(3):\n",
    "    plot_spectrum(energy, Y_demo[i], ax=ax2,\n",
    "                  title=f\"Example spectrum {i}  (logA={X_demo[i,0]:.2f}, Γ={X_demo[i,1]:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744f8b3-2da1-4035-abd8-f7d836bdf52a",
   "metadata": {},
   "source": [
    "Let's try the same with the Latin Hypercube Sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a3c53-3c05-4167-9154-cc00a1c4ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some simulated data \n",
    "X_lhs, Y_lhs = simulate_dataset_lhc(100, energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dd804-09e2-4969-96d1-02c90fac931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax1.scatter(X_lhs[:,0], X_lhs[:,1], s=2)\n",
    "\n",
    "for i in range(3):\n",
    "    plot_spectrum(energy, Y_lhs[i], ax=ax2,\n",
    "                  title=f\"Example spectrum {i}  (logA={X_lhs[i,0]:.2f}, Γ={X_lhs[i,1]:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9365f-fe50-4875-8da9-99be4b23de4e",
   "metadata": {},
   "source": [
    "Looks different, because logarithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509be8b",
   "metadata": {},
   "source": [
    "## Building a first surrogate: one node, one training example\n",
    "\n",
    "To start, we'll train the simplest neural network we can: a single node in a single layer.\n",
    "\n",
    "A “single node” (a single linear layer) with no hidden layers is:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = W\\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}$ has 2 inputs: $\\log(A)$, $\\Gamma$\n",
    "- $\\hat{\\mathbf{y}}$ has $n_E$ outputs (one per energy bin)\n",
    "\n",
    "This model has:\n",
    "- $2n_E$ weights + $n_E$ biases  \n",
    "For $n_E = 60$, that's 180 parameters.\n",
    "\n",
    "**Important**: in surrogate models, the word \"parameter\" is somewhat overloaded. This is because we're training the neural network to emulate the physical model's relationship between the model's input parameters (here the power law index and amplitude) and the model's output (here, the flux for each energy bin). However, the neural network *also* has parameters, which are often called *weights* and *biases*. These parameters we adjust during **training** such that the neural network produces outputs that closely resemble the spectrum we are trying to emulate. I'll try to make it clear from context which parameters we're talking about, but keep in mind that this might occasionally cause confusion as we go on. Machine learning also knows the concept of **hyperparameters**: these are properties of the neural network (e.g. the architecture, the number of layers/nodes, the details of the training process) that you can change in order to try and improve your neural network performance. We'll return to hyperparameters later.\n",
    "\n",
    "Training it on **one** example is guaranteed overfitting: this means that we already have many more parameters than we have data points in the training data.\n",
    "\n",
    "**Note**: Here, we call the collection of pairs of physics model parameters and physics model outputs (spectra) **\"training data\"**, even though they're not--strictly speaking--data in the sense of what we would usually consider data (i.e. observations coming from a telescope). In machine learning, any collection of input-output pairs used to train a machine learning algorithm is called \"training data\".\n",
    "\n",
    "We do it anyway to see what “overfitting” looks like in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tiniest dataset possible: 1 training example and a few validation examples\n",
    "X_train_tiny, Y_train_tiny = simulate_dataset_random(1, energy, noisy=False)\n",
    "X_val_tiny,   Y_val_tiny   = simulate_dataset_random(10, energy, noisy=False)\n",
    "\n",
    "# We'll scale inputs and outputs (this matters for neural nets!)\n",
    "# What that means we'll get into a little later\n",
    "x_scaler = StandardScaler().fit(X_train_tiny)\n",
    "y_scaler = StandardScaler().fit(Y_train_tiny)\n",
    "\n",
    "Xtr = x_scaler.transform(X_train_tiny).astype(np.float32)\n",
    "Ytr = y_scaler.transform(Y_train_tiny).astype(np.float32)\n",
    "\n",
    "Xva = x_scaler.transform(X_val_tiny).astype(np.float32)\n",
    "Yva = y_scaler.transform(Y_val_tiny).astype(np.float32)\n",
    "\n",
    "# Torch tensors\n",
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "Ytr_t = torch.from_numpy(Ytr)\n",
    "Xva_t = torch.from_numpy(Xva)\n",
    "Yva_t = torch.from_numpy(Yva)\n",
    "\n",
    "nE = Y_train_tiny.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb56f89-58bb-477b-9958-b87dd471d4db",
   "metadata": {},
   "source": [
    "Now we can write down our model. It's really simple at this point: just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325ebfe-3204-41e2-a1ff-9bdd612e2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our model\n",
    "tiny_model = nn.Linear(2, nE)  # single node per output dimension (linear map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe65bb-36fa-47ac-b591-6970c30e0201",
   "metadata": {},
   "source": [
    "The goal of the neural network is to predict the X-ray spectrum given some parameters. For this, we compare the neural network output to training examples (pairs of parameters and corresponding spectra) and move the parameters of the neural networks (the *weights* and *biases*) such that they produce output that is *more* similar to the training examples.\n",
    "\n",
    "To do this, we have to have a metric of comparison: how do we know when the neural network output is \"close\" to the training examples? This is done via a **loss function**, which quantifies the distance between the neural network output and the \"true\" training examples. Here, we're going to use the Mean Squared Error as a distance metric, which effectively just uses the square of the difference between the neural network output and the training examples to quantify distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388febc-b901-4b03-8975-a2a8b74550cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Mean Squared Error loss:\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5f086-f055-446f-9441-f754459f77d2",
   "metadata": {},
   "source": [
    "How do we know how to wiggle around the weights and biases in order to make the neural network produce outputs that ideally closely resemble the training examples? \n",
    "\n",
    "The answer here is optimization. It's worth mentioning here that neural network training rests on a concept called *backpropagation*, which effectively means that people figured out how to efficiently calculate *gradients* of the loss function with respect to the weights and biases (the parameters) of the neural network. This is the one weird trick that makes neural networks work in practice, and allows us to train models even when they have millions (or hundreds of millions) of parameters!\n",
    "\n",
    "There are a range of different optimization algorithms that machine learning researchers have developed to optimize neural networks, most of them based on an algorithm called **Stochastic Gradient Descent**. \n",
    "\n",
    "Here, we're using a version of that algorithm called ADAM (Adaptive Moment Estimation) as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3594a9-1596-48b6-8418-aab46e61b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a standard(ish) choice for optimization:\n",
    "opt = torch.optim.Adam(tiny_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a0883-a22c-4422-960d-6f96fe17f068",
   "metadata": {},
   "source": [
    "To train, we usually write a function that performs optimization for a number of fixed steps we call **epochs**. At each step, we compute the outputs of the neural network for our training examples, compare that to the outputs of the original model, compute the loss function, and then use the gradients to move the parameters of the neural network in the direction that decreases the loss function. Usually, in between, we also compute the loss function for examples that the neural network doesn't get to see during training, called the **validation set**. This set allows us to test how well the model **generalizes**, i.e. how well it does on data it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d976402-1c66-4353-8f30-fbaf6636f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, Xtr, Ytr, Xva, Yva, epochs=2000, print_every=400):\n",
    "    \"\"\"\n",
    "    Training function for neural network training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch NN model\n",
    "        The neural network model to use\n",
    "\n",
    "    Xtr, Ytr : arrays\n",
    "        The training data: Xtr are the input parameters of the \n",
    "        physical model, Ytr the spectra each set of parameters \n",
    "        generates\n",
    "\n",
    "    Xva, Yva: arrays\n",
    "        The validation data, of similar form as Xtr and Ytr\n",
    "\n",
    "    epochs : int\n",
    "        The number of iterations (epochs) to train for\n",
    "\n",
    "    print_every : int\n",
    "        How often to print training and validation performance\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_losses, val_losses : list, list\n",
    "        The loss at each epoch for both the training\n",
    "        and validation data sets\n",
    "    \"\"\"\n",
    "\n",
    "    # empty lists for training and validation losses\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # loop through each epoch:\n",
    "    for ep in range(1, epochs+1):\n",
    "\n",
    "        # train the model\n",
    "        model.train()\n",
    "\n",
    "        # set gradients to zero\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # predict the spectra for the training \n",
    "        # set parameters\n",
    "        pred = model(Xtr)\n",
    "\n",
    "        # calculate the loss between the predicted\n",
    "        # spectra and the training examples\n",
    "        loss = loss_fn(pred, Ytr)\n",
    "\n",
    "        # backpropagation of gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # take a step in the dimensions of the \n",
    "        # neural network parameters\n",
    "        opt.step()\n",
    "\n",
    "        # evaluate the model at the new location\n",
    "        model.eval()\n",
    "\n",
    "        # calculate validation losses\n",
    "        with torch.no_grad():\n",
    "            val = loss_fn(model(Xva), Yva)\n",
    "\n",
    "        # append losses to arrays\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val.item())\n",
    "\n",
    "        # print losses\n",
    "        if ep % print_every == 0:\n",
    "            print(f\"epoch {ep:4d}  train MSE={loss.item():.4e}  val MSE={val.item():.4e}\")\n",
    "    return np.array(train_losses), np.array(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf9ca5-f530-4f6d-9057-05b6d4c08e8d",
   "metadata": {},
   "source": [
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df254d21-4d3e-4d78-952a-e0ed154d8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l, val_l = train(tiny_model, Xtr_t, Ytr_t, Xva_t, Yva_t, epochs=500, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514caad2-a64e-4fdf-b37b-1828a5e396fd",
   "metadata": {},
   "source": [
    "Usually a good plot to make is a plot of the loss function as a function of epoch, for both the training and the validation data set. \n",
    "\n",
    "The training loss should always be smaller than the validation loss: it would be hard for the model to perform better on data that it hasn't seen than on data that it has! The one exception is if you include **drop-out** in your model (see suggestions for further reading at the end), where you *can* have a validation loss that's smaller than your training loss. \n",
    "\n",
    "Let's see what the loss function looks like for our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb72f7b-b7fe-4eb3-9ddd-9186eabd474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_l, label=\"train\")\n",
    "plt.plot(val_l, label=\"val\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE (scaled space)\")\n",
    "plt.title(\"Overfitting with 1 training example\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747d98d-4540-4428-99fc-bbd6be10ec67",
   "metadata": {},
   "source": [
    "So we see that the training loss is indeed smaller than the validation loss and going down. The validation loss is remaining high and not changing at all (that we can see). You could train this for longer (until your training loss stops going down), but let's take a look at what our neural network predicts for now:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd879e",
   "metadata": {},
   "source": [
    "### Inspect what the tiny model learned\n",
    "\n",
    "We'll compare:\n",
    "- the single training target vs prediction (it should fit very well)\n",
    "- a validation example vs prediction (it should be bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our tiny model\n",
    "tiny_model.eval()\n",
    "\n",
    "# fix the gradients\n",
    "with torch.no_grad():\n",
    "    # predict fluxes for training data\n",
    "    yhat_train = tiny_model(Xtr_t).numpy()\n",
    "    # predict fluxes for validation data\n",
    "    yhat_val0  = tiny_model(Xva_t[:1]).numpy()\n",
    "\n",
    "# invert scaling to counts space for training data\n",
    "yhat_train_counts = y_scaler.inverse_transform(yhat_train)[0]\n",
    "ytrain_counts     = Y_train_tiny[0]\n",
    "\n",
    "# invert scaling to counts space for validation data\n",
    "yhat_val_counts = y_scaler.inverse_transform(yhat_val0)[0]\n",
    "yval_counts     = Y_val_tiny[0]\n",
    "\n",
    "# plot the spectra\n",
    "plot_spectrum(energy, ytrain_counts, yhat_train_counts, title=\"Training example: target vs prediction\")\n",
    "plot_spectrum(energy, yval_counts,  yhat_val_counts,   title=\"Validation example: target vs prediction (fails)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da643123-1f2a-4c07-8379-efb3c33bb554",
   "metadata": {},
   "source": [
    "Okay, so for the training data, that looks pretty good. For the validation example, this looks terrible! This is expected! The neural network has only seen *one* single example of our training data, which means that it has no idea what the model function should look like for other parameter sets, and it will do terrible.\n",
    "\n",
    "**Exercise:** Try it with a different training example/validation example. You can also decide on a set of specific parameters you'd like to emulate. Do you get a better/different answer?\n",
    "\n",
    "The important lesson here is to **not use a neural network to extrapolate, but to interpolate**. Neural networks can be excellent interpolators, but they are *terrible* at extrapolating, which is what we're doing above. So in order to be able to interpolate well, we need more data. \n",
    "\n",
    "Oh, and one more point on the figure above: you might be able to tell that the neural network also seems to predict *negative* values. This is bad, because by definition, the model flux cannot be negative. So we'll have to do something about that, too. \n",
    "\n",
    "Trying this out with a single training example is fast, and it allows you to spot issues like these early on, before you get lost in complex architectures and spend weeks searching for bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd55291-a297-4a3d-a4e4-7db460b10f88",
   "metadata": {},
   "source": [
    "## Machine Learning: From Start To Finish\n",
    "\n",
    "Okay, so we've explored this on a single example, but that's not enough. A good exploratory process (especially when first getting started with machine learning), is to scale up both your neural network and training data set slowly: you keep adding training examples until your model can no longer do well on the training data, and then you add nodes/layers (or a different structure) to your neural network to increase its complexity (and therefore its flexibility in modelling complex functions). You keep doing this until you reach a model that does well over the full range of parameters you want to use it as a surrogate. \n",
    "\n",
    "Here, we're going to compress this somewhat for the purpose of this tutorial. Here's an overview of how to build a machine learning project:\n",
    "\n",
    "We'll now do the full workflow with a realistic dataset size:\n",
    "\n",
    "1. Decide the goal and target metric\n",
    "2. Generate / collect data\n",
    "3. Split into train/val/test\n",
    "4. Scale features (and sometimes targets)\n",
    "5. Choose a baseline model\n",
    "6. Train, monitor learning curves\n",
    "7. Tune hyperparameters (with cross-validation where appropriate)\n",
    "8. Evaluate on a held-out test set\n",
    "9. Interpret failure modes and report\n",
    "\n",
    "In spectroscopy terms: you should think of this like building a calibration pipeline (and if you ask David Hogg, he'll tell you that instrument calibration has basically always been a machine learning process, but that's a topic to discuss over drinks).\n",
    "\n",
    "### Deciding on a goal and target metric\n",
    "\n",
    "In our case, the overall goal is pretty clear: build a surrogate model that can approximate the physical model in generating an X-ray spectrum, but ideally much faster than the original model.\n",
    "\n",
    "However, there are subtleties:\n",
    "* How well does the neural network have to approximate the original model? Is 1% relative error enough? Does it have to be better?\n",
    "* Where and how can you tolerate biases in the model (and where can't you)?\n",
    "* In which parts of parameter space does the original model have issues (may be less precise, more biased, etc)? How will you treat those?\n",
    "\n",
    "These are questions that you'll want to consider when building your model, and be explicit about in the documentation/paper: where can people rely on your model well, and where can they not?\n",
    "\n",
    "### Generate/collect data\n",
    "\n",
    "Let's now generate more data. Above, I wrote two different functions to do so: one that samples uniformely from the parameter space within some bounds, and one that uses [Latin Hypercube Sampling](https://en.wikipedia.org/wiki/Latin_hypercube_sampling) to sample the space. In two dimensions, the two are fairly equivalent, but in higher dimensions, uniform sampling tends to produce clusters that we don't want (we ideally want to explore parameter space pretty broadly). Let's use Latin Hypercube Sampling to generate some training data.\n",
    "\n",
    "We'll also write a small function that turns the flux into log-flux, which we're going to train on. This ensures that our neural network cannot ever generate negative fluxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934acbe-17ef-444a-a8fe-d04af9774378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_targets(y):\n",
    "    \"\"\"\n",
    "    Transform targets (flux as a function of energy)\n",
    "    into logarithmic space for easier performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array\n",
    "        The array with flux in linear space\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_log : array\n",
    "        The array with flux in log space\n",
    "\n",
    "    \"\"\"\n",
    "    y_log = np.log(y)\n",
    "    # log1p is stable for small counts and handles 0 gracefully\n",
    "    return y_log\n",
    "\n",
    "def inverse_transform_targets(y_log):\n",
    "    \"\"\"\n",
    "    Inverse-transform targets (flux as a function of energy)\n",
    "    from logarithmic space (back) into linear space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_log : array\n",
    "        The array with flux in log space\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : array\n",
    "        The array with flux in linear space\n",
    "    \"\"\"\n",
    "    return np.exp(y_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84fb10-eeaf-4368-b671-0f995c693c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training examples: we will experiment with this later!\n",
    "n_samples = 5000\n",
    "\n",
    "# simulate the data \n",
    "X, Y_linear = simulate_dataset_random(n_samples, energy, noisy=False)\n",
    "\n",
    "# transform fluxes into log-space\n",
    "Y = transform_targets(Y_linear).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc90766",
   "metadata": {},
   "source": [
    "## Training, validation, and test sets\n",
    "\n",
    "In real projects you typically want **three** disjoint sets:\n",
    "\n",
    "- **Training set:** Fit the model parameters (weights/biases). This is the data we're training the model on.\n",
    "- **Validation set:** This is data that the model doesn't see during learning, and we use it like we did above to check how the model is doing, figure out bugs (like the fact that currently our model is allowed to predict negative fluxes), and also tune hyperparameters. Here, hyperparameters are the details of the neural network (its architecture, like the number of nodes and layers), of the optimization (e.g. the learning rate parameter some optimization algorithms use) and other features such as dropout, the batch size, etc.\n",
    "- **Test set:** We will also generate a test set, which we do not touch at all during training/validation. We will reserve it for reporting the final performance at the end. These are the numbers that go into the paper. But we already have a validation set, you say? Yes, but there's a subtle, but important point here: while we don't use the validation set directly in training, we do use it to make decisions about the model and the training algorithms. In this way, the model indirectly does \"see\" the validation data, and it has been shown to be possible to overfit on the validation data. The test set is to make sure we get an unbiased, independent measure of our performance at the end.\n",
    "\n",
    "**Important:**  \n",
    "You should not repeatedly look at the test set while iterating, or it stops being a good (unbiased) test.\n",
    "\n",
    "We can use a function from the library `scikit-learn` to generate a train-test split. We'll do this twice: once to first split out the dataset we'll train on, and from the remainder, we'll split out our test set (which we'll put aside until the end), and the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6248561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split: 70/15/15\n",
    "X_train, X_tmp, Y_train, Y_tmp = train_test_split(X, Y, test_size=0.30, random_state=123)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_tmp, Y_tmp, test_size=0.50, random_state=123)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b3025",
   "metadata": {},
   "source": [
    "### Scaling features (and targets)\n",
    "\n",
    "Neural networks are sensitive to feature scaling. This means that they don't work very well when the input or output dimensions are orders of magnitude different to each other. For example, our power law index is relatively tightly constrained between 1 and 5, whereas the amplitude can vary over multiple orders of magnitude. This can be hard for neural networks, whose weights are usually initialized between 0 and 1, to learn, so we'll make it easier on our neural network and scale our **features** (the inputs to the neural networks, here the parameters of the physical model) and the **targets** (the output of the neural network, here the model spectra).\n",
    "\n",
    "A common practice is to scale features such that they have a mean of zero and a variance of once. Note that this scaling is applied to the spectrum on a per-energy-bin basis, which makes the spectrum look *really* weird during training, but because it's an easily reversible operation, it's not a concern for our actual model prediction.\n",
    "\n",
    "There are many other scalers (see e.g. [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)), and which one you use heavily depends on your problem. There are also more advanced options such as [Fourier Feature Encoding](https://sair.synerise.com/fourier-feature-encoding/) (see also [this paper](https://bmild.github.io/fourfeat/)) that can help you with your neural network performance. This is a huge rabbit hole to fall down into, which we're not going to do today.\n",
    "\n",
    "That that you should fit scalers **only on the training set** and apply to validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a04ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler for the inputs\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# scaler for the outputs\n",
    "y_scaler = StandardScaler().fit(Y_train)\n",
    "\n",
    "# transform train/val/test features\n",
    "Xtr = x_scaler.transform(X_train).astype(np.float32)\n",
    "Xva = x_scaler.transform(X_val).astype(np.float32)\n",
    "Xte = x_scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# transform train/val/test targets\n",
    "Ytr = y_scaler.transform(Y_train).astype(np.float32)\n",
    "Yva = y_scaler.transform(Y_val).astype(np.float32)\n",
    "Yte = y_scaler.transform(Y_test).astype(np.float32)\n",
    "\n",
    "nE = Ytr.shape[1]\n",
    "print(\"nE:\", nE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883c0f7",
   "metadata": {},
   "source": [
    "### Baseline model: linear surrogate\n",
    "\n",
    "Before reaching for “deep learning”, we'll start with something that's easier to understand.\n",
    "\n",
    "A linear model cannot represent everything, but here it might do surprisingly well because\n",
    "a power law is “simple”. This is useful as a *sanity baseline*.\n",
    "\n",
    "We'll train:\n",
    "- linear surrogate (same as before, but with enough data)\n",
    "- monitor learning curves\n",
    "\n",
    "#### Some housekeeping\n",
    "\n",
    "When we have large data sets, the whole data set will not fit into the memory of your GPU. In these cases, we train in **batches**: During each epoch, the training data set is split up randomly into smaller batches of $M$ training examples (often, some multiple of 2, like 64 or 128; doing it *not* in multiples of two is a great way to get some computer scientists **really** angry). In each epoch, you iteratively run through all of your batches first, then start the next training epoch. This has the advantage of often being significantly faster and computationally less demanding. You also make your optimization more noisy. Surprisingly, this can actually *help* with better training, because you might avoid local minima better than if you trained on all of your training data simultaneously.\n",
    "\n",
    "`pytorch`, the library we're using today, has a bunch of infrastructure that abstracts a lot of that away from you, if you put your training data into the right format. Specifically, the `Dataset` and `DataLoader` classes are really helpful in making that work reliably (see the tutorial [here](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html). \n",
    "\n",
    "Let's put our data into the right format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(X, Y, batch_size=128, shuffle=True):\n",
    "    \"\"\"\n",
    "    Take our data and turn it into a DataLoader object\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : array, array\n",
    "        The (training) data features and targets\n",
    "\n",
    "    batch_size : int, default 128\n",
    "        The size for each training batch\n",
    "\n",
    "    shuffle : bool, default True\n",
    "        If true, shuffle the data into new batches \n",
    "        for every epoch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dl : pytorch.DataLoader object\n",
    "        The DataLoader with the training/test/validation data\n",
    "    \"\"\"\n",
    "    # first put data into a `DataSet` object\n",
    "    ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(Y))\n",
    "\n",
    "    # now build DataLoader\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb684e-52ea-4de0-b618-58d14ac0829a",
   "metadata": {},
   "source": [
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7197bc-0b78-45a3-bfb7-735eccd980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_dataloader(Xtr, Ytr, batch_size=256, shuffle=True)\n",
    "val_loader   = make_dataloader(Xva, Yva, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d9f9b-3237-4e05-abee-42b42ae4f401",
   "metadata": {},
   "source": [
    "Now we have to rewrite our training loop in order to include the DataLoaders. We'll also include a couple of nifty machine learning tricks:\n",
    "\n",
    "* we'll save the best-performing model so that we can pull it out again later\n",
    "* we'll also use **early stopping**: this is a concept where we want to stop training when the performance on the *validation data set* no longer improves, because then we're moving into the realm of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9d292-487f-42c6-9c96-d374c3d9ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, train_loader, val_loader, epochs=200, patience=20,\n",
    "                print_every=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Training loop for training our neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch NN model\n",
    "        The neural network model\n",
    "\n",
    "    train_loader, val_loader : pytorch.DataLoader objects\n",
    "        The data loader objects with our training/validation data\n",
    "\n",
    "    epochs : int, default 200\n",
    "        The number of epochs to run for\n",
    "\n",
    "    patience : int, default 20\n",
    "        The number of epochs to train without improvement\n",
    "        before stopping\n",
    "\n",
    "    print_every : int, default 50\n",
    "        Print losses every `print_every` epochs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hist : dict\n",
    "        A dictionary with the training and validation losses\n",
    "    \"\"\"\n",
    "    # initialize best validation performance and model state\n",
    "    best_val = np.inf\n",
    "    best_state = None\n",
    "\n",
    "    # counter for the number of validation losses worse \n",
    "    # than the best one\n",
    "    bad = 0\n",
    "\n",
    "    # initialize dictionary for storing the losses\n",
    "    hist = {\"train\": [], \"val\": []}\n",
    "\n",
    "    # run through epochs\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tl=[]\n",
    "\n",
    "        # iterate through the training DataLoader (i.e. over batches)\n",
    "        for xb, yb in train_loader:\n",
    "            # zero out the gradients\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # predict the model fluxes\n",
    "            pred = model(xb)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # step in parameter space\n",
    "            opt.step()\n",
    "            tl.append(loss.item())\n",
    "\n",
    "        # store mean loss over all batches as \n",
    "        # the training loss\n",
    "        train_loss = float(np.mean(tl))\n",
    "\n",
    "        # run through validation batches and \n",
    "        # compute mean loss\n",
    "        model.eval()\n",
    "        vl=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                vl.append(loss_fn(model(xb), yb).item())\n",
    "        val_loss = float(np.mean(vl))\n",
    "\n",
    "        hist[\"train\"].append(train_loss)\n",
    "        hist[\"val\"].append(val_loss)\n",
    "        \n",
    "        # print losses\n",
    "        if ep % print_every == 0:\n",
    "            print(f\"epoch {ep:4d}  train MSE={train_loss:.4e}  val MSE={val_loss:.4e}\")\n",
    "\n",
    "\n",
    "        # if validation loss is better than previous \n",
    "        # best value, then keep this as the new best model\n",
    "        # and reset \"bad model\" counter\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().clone() for k,v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            # if we've had more bad models than our `patience` parameter\n",
    "            # stop training, because it ain't getting better!\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4e1b2-8e5d-483f-aa60-37bbf6435e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our linear model\n",
    "linear_model = nn.Linear(2, nE)\n",
    "\n",
    "# set the learning rate for the optimizer\n",
    "lr = 1e-3\n",
    "\n",
    "# same optimizer as previously\n",
    "opt = torch.optim.Adam(linear_model.parameters(), lr=lr)\n",
    "\n",
    "# same loss function as previously\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#let's train!\n",
    "hist_lin = train_epochs(linear_model, train_loader, val_loader, epochs=200, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84537d-bb6a-4258-ad5c-9d4a078c7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss function for the training and validation data\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_lin[\"train\"], label=\"train\")\n",
    "plt.plot(hist_lin[\"val\"], label=\"val\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE (scaled log-count space)\")\n",
    "plt.title(\"Learning curves: linear surrogate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa32a11-d151-43ff-bbfc-e917cd149975",
   "metadata": {},
   "source": [
    "We're going to make a function that plots an example spectrum, the neural network prediction, and also the absolute residuals, defined as\n",
    "\n",
    "$$\n",
    "r = \\frac{|y - \\hat{y}|}{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763acc19-8749-42d6-bcf5-b121e0b1461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(energy, ytrue, ypred, title=None):\n",
    "    \"\"\"\n",
    "    Plot an example of the data and the neural network prediction,\n",
    "    as well as the relative error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energy : array\n",
    "        The array of energies\n",
    "\n",
    "    ytrue : array\n",
    "        The array with the true model values\n",
    "\n",
    "    ypred : array\n",
    "        The array with the model values predicted \n",
    "        by the neural network\n",
    "\n",
    "    ax : plt.Axes object\n",
    "        a matplotlib object to plot the results into\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax : plt.Axes object\n",
    "        a matplotlib object to plot the results into\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6,6), height_ratios=(3,1), sharex=True)\n",
    "\n",
    "    ax1.plot(energy, ytrue, lw=1, marker=\"^\", color=\"black\", markersize=3, label=\"true model\")\n",
    "    ax1.plot(energy, ypred, lw=1, marker=\"o\", color=sns.color_palette()[1], markersize=3, label=\"NN prediction\")\n",
    "    ax1.set_ylabel(\"Flux [arbitrary units]\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "\n",
    "    rel_error = np.abs(ytrue - ypred)/ytrue\n",
    "    ax2.plot(energy, rel_error, lw=1, color=\"black\", label=\"relative error\")\n",
    "    ax2.axhline(0.0, lw=2, color=\"red\")\n",
    "    ax2.set_xlabel(\"Energy [keV]\")\n",
    "    if title:\n",
    "        ax1.set_title(title)\n",
    "    \n",
    "    return fig, ax1, ax2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffafd7-b476-4aa3-b6b1-299369a708e3",
   "metadata": {},
   "source": [
    "Let's plot some examples: you can re-run the cell below multiple times to look at different random samples from the training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3543-67c2-47f3-ae35-b7db47ba2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "linear_model.eval()\n",
    "\n",
    "# fix gradients\n",
    "with torch.no_grad():\n",
    "    yhat_train = linear_model(torch.from_numpy(Xtr)).numpy()\n",
    "    yhat_val  = linear_model(torch.from_numpy(Xva)).numpy()\n",
    "\n",
    "# number of training and validation examples\n",
    "ntrain = Xtr.shape[0]\n",
    "nval = Xva.shape[0]\n",
    "\n",
    "# pick random indices for plotting\n",
    "idx_train = np.random.randint(0, ntrain)\n",
    "idx_val = np.random.randint(0, nval)\n",
    "\n",
    "# invert scaling to counts space\n",
    "yhat_train_counts = y_scaler.inverse_transform(yhat_train)[idx_train]\n",
    "ytrain_counts     = Y_train[idx_train]\n",
    "\n",
    "yhat_val_counts = y_scaler.inverse_transform(yhat_val)[idx_val]\n",
    "yval_counts     = Y_val[idx_val]\n",
    "\n",
    "\n",
    "plot_example(energy, np.exp(ytrain_counts), np.exp(yhat_train_counts), title=\"Training example: target vs prediction\")\n",
    "plot_example(energy, np.exp(yval_counts),   np.exp(yhat_val_counts),   title=\"Validation example: target vs prediction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529f2c8-884b-4443-9909-3acc58f4684f",
   "metadata": {},
   "source": [
    "Okay, so that looks surprisingly good! Even a linear model can apparently approximate the power law pretty well. \n",
    "\n",
    "## Nonlinear surrogate: add a hidden layer\n",
    "\n",
    "To emulate more complex forward models, you generally need nonlinearity.\n",
    "A minimal neural network for regression is:\n",
    "\n",
    "`(inputs) -> Linear -> Activation function -> Linear -> (outputs)`\n",
    "\n",
    "The linear layer in the middle is called a \"hidden layer\", and the more of these you add, the easier it will be to model non-linearity. Every time you add a layer, you also add parameters, so be careful: your model can very quickly become very big (however, this is often not an issue in machine learning, as long as you have enough training data and enough computational time).\n",
    "\n",
    "A neural network of this type is also traditionally called a **Multi-Layer Perceptron** (MLP). If you add more than, say, 2-3 layers, this is what people generally call **deep learning**. \n",
    "\n",
    "In the context of making our model non-linear, we need to talk about another concept: **activation functions**. Remember that our linear model from earlier was of the type:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = W\\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "What we're going to do now is actually stack two of those together, such that:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\sigma(W_1\\mathbf{x} + \\mathbf{b_1})\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = W_2\\mathbf{a} + \\mathbf{b_2}\n",
    "$$\n",
    "\n",
    "Here we've done two things. First, we've stacked two linear models together, such that the first layer produces intermediate values $a$ and the second layer relates these intermediate values $a$ to the outputs we want to predict $\\hat{\\mathbf{y}}$. You'll notice that there's this function $\\sigma()$ in there as well. This is the activation function. It takes our linear model and makes it non-linear, by explicitly squashing the outputs of the linear layer through a non-linearity. A function that people were using early on (and that's still very useful in some cases) is the **sigmoid function**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "The sigmoid function can cause some issues in deep networks, so there are a wide range of alternatives to use. A typical one is called the **Rectified Linear Unit (ReLU)**, which is simply zero below zero and a linear function above, $\\sigma(z) = \\mathrm{max}(0, z)$.\n",
    "\n",
    "\n",
    "Let's write our model into a class in typical PyTorch syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e43f30-4f0e-4bdc-8b62-1cf10b253332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class can be any name, but should inherit from `nn.Module`\n",
    "class MLP(nn.Module):\n",
    "    # all classes need an `__init__` method, here this allows you to \n",
    "    # set the dimension of the outputs and the number of nodes in the hidden layer\n",
    "    def __init__(self, in_dim=2, out_dim=60, hidden=64):\n",
    "        \"\"\"\n",
    "        Initialize neural network class. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_dim : int\n",
    "            The number of input dimensions (i.e. number of \n",
    "            parameters in the physics model)\n",
    "\n",
    "        out_dim : int\n",
    "            The number of output dimensions (i.e. the number \n",
    "            of fluxes, corresponding to the number of energy \n",
    "            bins in the spectrum)\n",
    "\n",
    "        hidden : int\n",
    "            The number of nodes in the hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # nn.Sequential is a container for a sequence of modules, \n",
    "        # here linear layers and activation functions\n",
    "        self.net = nn.Sequential(\n",
    "            # weights going from inputs to hidden layer\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            # activation on hidden nodes\n",
    "            nn.ReLU(),\n",
    "            # weights going from hidden layer to outputs\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the neural network.\n",
    "        Means you stick in parameters and get a predicted\n",
    "        spectrum out.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array\n",
    "            The model inputs (i.e. the parameters for which you want\n",
    "            a model)\n",
    "\n",
    "        \"\"\"\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19605147-0097-42df-9ca1-df5d1fe5d943",
   "metadata": {},
   "source": [
    "We can now initialize and train it the same way we did our previous network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed38fc-a82a-47cb-a994-82f3d4258e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our linear model\n",
    "mlp = MLP(in_dim=2, out_dim=nE, hidden=128)\n",
    "\n",
    "# set the learning rate for the optimizer\n",
    "lr = 1e-3\n",
    "\n",
    "# same optimizer as previously\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "\n",
    "# same loss function as previously\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#let's train!\n",
    "hist_mlp = train_epochs(mlp, train_loader, val_loader, epochs=3000, patience=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06b82f-1661-4084-babc-1639a52fe634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss functions for training and validation data sets:\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_mlp[\"train\"], label=\"train\")\n",
    "plt.plot(hist_mlp[\"val\"], label=\"val\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE (scaled log-count space)\")\n",
    "plt.title(\"Learning curves: linear surrogate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409b86c-c544-4de9-b3f6-2dda8ab0fd79",
   "metadata": {},
   "source": [
    "Let's look at some examples again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5591a0e-b5fb-4d1d-b528-c15b35cb948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat_train = mlp(torch.from_numpy(Xtr)).numpy()\n",
    "    yhat_val  = mlp(torch.from_numpy(Xva)).numpy()\n",
    "\n",
    "\n",
    "ntrain = Xtr.shape[0]\n",
    "nval = Xva.shape[0]\n",
    "\n",
    "idx_train = np.random.randint(0, ntrain)\n",
    "idx_val = np.random.randint(0, nval)\n",
    "\n",
    "# invert scaling to counts space\n",
    "yhat_train_counts = y_scaler.inverse_transform(yhat_train)[idx_train]\n",
    "ytrain_counts     = np.exp(Y_train[idx_train])\n",
    "\n",
    "yhat_val_counts = y_scaler.inverse_transform(yhat_val)[idx_val]\n",
    "yval_counts     = np.exp(Y_val[idx_val])\n",
    "\n",
    "plot_example(energy, ytrain_counts, np.exp(yhat_train_counts), title=\"Training example: target vs prediction\")\n",
    "plot_example(energy, yval_counts,   np.exp(yhat_val_counts),   title=\"Validation example: target vs prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae5782-8a9e-4234-b049-39450cf1a1da",
   "metadata": {},
   "source": [
    "That looks very good, but not perfect. Depending on your example, you might get a prediction that's systematically a few percent off. \n",
    "\n",
    "### How do biases affect physical inference?\n",
    "\n",
    "Ultimately, what we need to know (and what we want to evaluate performance on, is whether a biased surrogate model leads to biased inferences.\n",
    "\n",
    "Let's give this a try, and run a quick bit of MCMC. First, let's simulate a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d1034-c001-4e0c-95b1-a18065197433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a reminder, these are the ranges within which we \n",
    "# simulated training data\n",
    "# !! IMPORTANT: do not try to infer parameters outside this range !!\n",
    "amp_range = (5e-4, 5e-2) \n",
    "gamma_range = (1.0, 3.0)\n",
    "\n",
    "# true parameters we're going to simulate a dataset from\n",
    "# we're going to work with log-amplitude rather than amplitude\n",
    "# for easier sampling\n",
    "true_pars = [np.log(1e-3), 1.2]\n",
    "exposure = 25000.0\n",
    "\n",
    "# need to crank up exposure time otherwise the counts will be zero\n",
    "flux = powerlaw_counts(energy, np.exp(true_pars[0]), true_pars[1], e0=1.0, exposure=exposure)\n",
    "counts = np.random.poisson(flux)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "ax.plot(energy, flux, color=\"black\", label=\"Flux without noise\")\n",
    "ax.errorbar(energy, counts, yerr=np.sqrt(counts), marker=\"o\", ls=\"\", markersize=3, label=\"counts\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Energy [keV]\")\n",
    "ax.set_ylabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2451903-7ffa-469c-acb9-985436cb741d",
   "metadata": {},
   "source": [
    "Now we have to define a likelihood and some priors to actually do the sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778001a-df13-4fec-80b7-75b2bdb60858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(pars):\n",
    "    \"\"\"\n",
    "    log-prior for the parameters. We're going to \n",
    "    pick flat priors within the bounds within which we've\n",
    "    simulated our training data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : iterable\n",
    "        a set of parameters, of the form [log_amp, gamma]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_prior : float\n",
    "        The logarithm of the prior probability distribution\n",
    "        for the parameters in `pars`. Outside of prior bounds, \n",
    "        will return `-np.inf`    \n",
    "    \"\"\"\n",
    "    log_amp = pars[0]\n",
    "    gamma = pars[1]\n",
    "\n",
    "    if log_amp < np.log(5e-4) or log_amp > np.log(5e-2) or gamma < 1.0 or gamma > 3.0:\n",
    "        return -np.inf\n",
    "\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def loglikelihood(pars, energy, counts):\n",
    "    \"\"\"\n",
    "    Poisson log-likelihood for a power law model \n",
    "    with parameters `amp` and `gamma`.\n",
    "\n",
    "    Uses the neural network emulator. \n",
    "    This function will first transform the parameters \n",
    "    in `pars` to the scaled space in which the neural network\n",
    "    operates, predict the model spectrum given those parameters \n",
    "    and transform that model spectrum from the neural-network \n",
    "    space into counts space\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : iterable\n",
    "        a set of parameters, of the form [log_amp, gamma]\n",
    "\n",
    "    energy : array\n",
    "        The array with photon energy bins\n",
    "\n",
    "    counts : array\n",
    "        The array with (poisson-distributed) photon counts \n",
    "        for each of the bins in `energy`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_likelihood : float\n",
    "        The logarithm of the Poisson likelihood for \n",
    "        the power law model, the parameters in `pars` \n",
    "        and the counts in `counts`\n",
    "    \"\"\"\n",
    "    X_pars = x_scaler.transform(pars.reshape(1, -1))\n",
    "    X_pars = torch.from_numpy(np.array(X_pars, dtype=np.float32))\n",
    "    \n",
    "    mpred = mlp(X_pars).detach().numpy().reshape(1, -1)\n",
    "    model_flux = np.exp(y_scaler.inverse_transform(mpred)) * exposure\n",
    "\n",
    "    loglike = np.sum(-model_flux + counts * np.log(model_flux) - scipy_gammaln(counts + 1.0))\n",
    "\n",
    "    if not np.isfinite(loglike):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return loglike\n",
    "\n",
    "def logposterior(pars, energy, counts):\n",
    "    \"\"\"\n",
    "    log-posterior function\n",
    "    \"\"\"\n",
    "    return logprior(pars) + loglikelihood(pars, energy, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0b55f-5d25-48e1-a9b3-0e84b3f28a7d",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcd959-c990-4ca8-bd8d-2903e8770fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log prior with the true parameters\n",
    "print(logprior(true_pars))\n",
    "\n",
    "# log prior with bad parameters\n",
    "print(logprior([-10, 5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48257e4-5e98-4edd-a495-b3796632b204",
   "metadata": {},
   "source": [
    "Let's sample this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d4bbd-f642-4754-88f9-abe39550f6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 2, 100\n",
    "ivar = 1. / np.random.rand(ndim)\n",
    "p0 = true_pars + np.random.randn(nwalkers, ndim)*0.01*true_pars\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior, args=[energy, counts])\n",
    "_, _, _ = sampler.run_mcmc(p0, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9059f2-e69c-4a95-8d00-e1ecfa632913",
   "metadata": {},
   "source": [
    "Let's make some trace plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417a460-0bda-42cd-97a1-2ec0bab35afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.flatchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1a595-4331-4039-a580-df9e7705f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "ax1.plot(samples[:,0], lw=1, color=\"black\")\n",
    "ax1.axhline(true_pars[0], lw=2, color=\"red\")\n",
    "\n",
    "ax2.plot(samples[:,1], lw=1, color=\"black\")\n",
    "ax2.axhline(true_pars[1], lw=2, color=\"red\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be2db3-ef55-4a3e-8130-59bde664ebbf",
   "metadata": {},
   "source": [
    "Let's make a corner plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17295a-8d81-45a3-97be-9363935a01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corner.corner(samples, truths=true_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204818b-c581-48c5-b297-b5bc1495f264",
   "metadata": {},
   "source": [
    "So just from those plots, it looks like it might be a little bit biased, but we can't necessarily tell from a single example. The posterior for a given parameter set and realization *should* randomly vary around the true value.\n",
    "\n",
    "And posterior predictive plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77167a-e985-4e1b-b457-faaf2adda4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f20130-7e11-4bde-9209-710664df7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive(samples, energy, counts, nsamples=20, ax = None, exposure=1):\n",
    "    \"\"\"\n",
    "    Make a posterior predictive plot\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "\n",
    "    ax.errorbar(energy, counts, yerr=np.sqrt(counts), marker=\"o\", ls=\"\",\n",
    "                color=\"black\", markersize=3, label=\"counts\")\n",
    "\n",
    "\n",
    "    idx_all = np.random.choice(np.arange(0, samples.shape[0], 1, dtype=int), size=nsamples, replace=False)\n",
    "\n",
    "    for i,idx in enumerate(idx_all):\n",
    "        # get a single set of parameters from posterior\n",
    "        p = samples[idx]\n",
    "\n",
    "        # cast parameters into scaled version for neural network\n",
    "        X_pars = x_scaler.transform(p.reshape(1, -1))\n",
    "        X_pars = torch.from_numpy(np.array(X_pars, dtype=np.float32))\n",
    "\n",
    "        # predict model flux, then rescale to original flux units\n",
    "        mpred = mlp(X_pars).detach().numpy().reshape(1, -1)\n",
    "        model_flux = np.exp(y_scaler.inverse_transform(mpred)) * exposure\n",
    "\n",
    "        # random draws from Poisson distribution to simulate data sets from posterior\n",
    "        model_counts = np.random.poisson(model_flux)\n",
    "        if i == 0:\n",
    "            m1_legend = \"draws from original model\"\n",
    "            m2_legend = \"draws from neural network\"\n",
    "        else:\n",
    "            m1_legend = \"\"\n",
    "            m2_legend = \"\"\n",
    "\n",
    "        ax.scatter(energy, model_counts, color=sns.color_palette()[1], alpha=0.3, label=m2_legend)\n",
    "\n",
    "        # generate the model spectrum with the original \"physics\" model\n",
    "        mf_physics = powerlaw_counts(energy, np.exp(p[0]), p[1], exposure=exposure)\n",
    "        # include Poisson data\n",
    "        mc_physics = np.random.poisson(mf_physics)\n",
    "        ax.scatter(energy, mc_physics, color=\"purple\", alpha=0.1, label=m1_legend)\n",
    "\n",
    "\n",
    "    ax.set_xlabel(\"Energy [keV]\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a9b45-1dd1-4074-9647-7673325e39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_predictive(samples, energy, counts, exposure=exposure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16ed05-c0b6-4b30-ac40-2b42d2cf4cc6",
   "metadata": {},
   "source": [
    "Okay, great! That seems to work! Hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1b0e0",
   "metadata": {},
   "source": [
    "### Evaluate on the test set\n",
    "\n",
    "Now we use the test set **once** to quantify generalization.\n",
    "We'll compute:\n",
    "- MSE in scaled space (what we trained on)\n",
    "- and a more interpretable metric: median absolute fractional error in counts space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f455c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_counts(model, X_scaled):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        yhat_scaled = model(torch.from_numpy(X_scaled)).numpy()\n",
    "    yhat_log = y_scaler.inverse_transform(yhat_scaled)\n",
    "    yhat_counts = inverse_transform_targets(yhat_log)\n",
    "    return yhat_counts\n",
    "\n",
    "# Predict on test set\n",
    "yhat_counts_lin = predict_counts(linear_model, Xte)\n",
    "\n",
    "# True counts for test (unscaled) - recover from Y_test (log space)\n",
    "ytrue_counts = inverse_transform_targets(Y_test)\n",
    "\n",
    "mse_counts = np.mean((yhat_counts_lin - ytrue_counts)**2)\n",
    "frac_err = np.abs(yhat_counts_lin - ytrue_counts) / ytrue_counts\n",
    "med_frac = np.median(frac_err)\n",
    "\n",
    "print(\"Test MSE in counts space:\", mse_counts)\n",
    "print(\"Median |fractional error|:\", med_frac)\n",
    "\n",
    "# Plot a few random test cases\n",
    "idx = rng.choice(len(X_test), size=3, replace=False)\n",
    "for j,i in enumerate(idx):\n",
    "    plot_spectrum(energy, ytrue_counts[i], yhat_counts_lin[i],\n",
    "                  title=f\"Linear surrogate on test example {j}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c41a6f",
   "metadata": {},
   "source": [
    "## Broken Power Laws\n",
    "\n",
    "A slightly more difficult (toy) model: a broken power law.\n",
    "\n",
    "We choose:\n",
    "- energy grid: 0.3–10 keV\n",
    "- pivot energy: 1 keV (just for context)\n",
    "- **broken broken power-law shape parameters**: low-energy photon index $\\Gamma_1$, high-energy photon index $\\Gamma_2$, and break energy $E_b$\n",
    "\n",
    "We will *learn* the mapping:\n",
    "\n",
    "$$\n",
    "(\\Gamma_1, \\Gamma_2, \\log_{10} E_b) \\rightarrow \\text{spectrum}(E)\n",
    "$$\n",
    "\n",
    "**Important:** we do **not** train on an overall normalization/amplitude. Instead we normalize each simulated spectrum to **unit total counts**. That means the network learns only the **shape**, not the amplitude.\n",
    "\n",
    "In real fitting, you can often recover amplitude later by fitting a single scalar multiplier at the same time as the shape parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broken_powerlaw_counts(energy, gamma1, gamma2, eb, amp=1.0, exposure=1.0):\n",
    "    \"\"\"Deterministic toy counts spectrum for a *broken* power law (no noise).\n",
    "\n",
    "    The photon flux is continuous at the break energy Eb by construction:\n",
    "\n",
    "        F(E) = Ab * (E/Eb)^(-Gamma1)   for E < Eb\n",
    "             = Ab * (E/Eb)^(-Gamma2)   for E >= Eb\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energy : array, shape (nE,)\n",
    "        Energy grid in keV.\n",
    "    gamma1, gamma2 : float\n",
    "        Photon indices below/above the break.\n",
    "    eb : float\n",
    "        Break energy in keV.\n",
    "    amp : float\n",
    "        Normalization defined at E = Eb (kept fixed here; we do NOT train on amplitude).\n",
    "    exposure : float\n",
    "        Exposure time in arbitrary units.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts : array, shape (nE,)\n",
    "        Expected counts per energy bin (arbitrary units).\n",
    "    \"\"\"\n",
    "    flux = np.where(energy < eb, (energy/eb)**(-gamma1), (energy/eb)**(-gamma2))\n",
    "    photon_flux = amp * flux\n",
    "    counts = photon_flux * exposure\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c9556-a8f4-4401-afa1-810d6e6b4154",
   "metadata": {},
   "source": [
    "### Simulating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1de98f-da83-4cc7-8f8e-dba34193ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bpl_dataset_lhc(n_samples, energy,\n",
    "                         gamma1_range=(0.0, 1.5),\n",
    "                         gamma2_range=(1.5, 5.0),\n",
    "                         eb_range=(0.5, 5.0),\n",
    "                         exposure=1.0,\n",
    "                         noisy=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Simulate a dataset of (parameters -> spectrum), \n",
    "    using a Latin Hypercube Sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        The number of samples to draw in each dimension. Full dataset will be \n",
    "        of size nsamples\n",
    "\n",
    "    energy : array\n",
    "        The energy grid to simulate the data over\n",
    "\n",
    "    logamp_range : (float, float)\n",
    "        The lower and upper boundary between which to sample \n",
    "        the amplitude\n",
    "\n",
    "    gamma_range : (float, float)\n",
    "        The lower and upper boundary between which to sample \n",
    "        the power law index\n",
    "\n",
    "    exposure : float\n",
    "        The exposure of the observation\n",
    "\n",
    "    noisy : bool, default False\n",
    "        If True, add Poisson noise to the spectrum\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array of shape (nsamples, 2)\n",
    "        The array containing all the pairs of parameters\n",
    "\n",
    "    Y : array of (nsamples, len(energy))\n",
    "        The array with all simulated spectra \n",
    "    \"\"\"\n",
    "    # set up Latin Hypercube Sampling\n",
    "    sampler = qmc.LatinHypercube(d=3)\n",
    "\n",
    "    # sample randomly using LHS\n",
    "    sample = sampler.random(n=n_samples)\n",
    "\n",
    "    # set array of lower and upper bounds\n",
    "    l_bounds = [gamma1_range[0], gamma2_range[0], eb_range[0]]\n",
    "    u_bounds = [gamma1_range[1], gamma2_range[1], eb_range[1]]\n",
    "\n",
    "    # scale sample to bounds\n",
    "    X = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "    Y = np.stack([broken_powerlaw_counts(energy, gamma1_i, gamma2_i, eb_i, amp=1.0, exposure=1.0) for gamma1_i, gamma2_i, eb_i in X]).astype(np.float32)\n",
    "\n",
    "    if noisy:\n",
    "        # Poisson noise around the expected counts\n",
    "        Y = rng.poisson(Y).astype(np.float32)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7eb53-dd1d-44f3-8f5a-12b0bed831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some simulated data \n",
    "X_lhs, Y_lhs = simulate_bpl_dataset_lhc(5000, energy)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax1.scatter(X_lhs[:,0], X_lhs[:,1], s=2)\n",
    "\n",
    "for i in range(3):\n",
    "    plot_spectrum(energy, Y_lhs[i], ax=ax2,\n",
    "                  title=f\"Example spectrum {i}  (logA={X_lhs[i,0]:.2f}, Γ={X_lhs[i,1]:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143ae53-8018-40b5-9f8e-23fae24c6ede",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Okay, we'll have to do similar preprocessing as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdca9e-ac62-4ae7-9621-f3b6a02ba0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform targets to log flux\n",
    "Y = transform_targets(Y_lhs).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20412062-0ff7-471f-aa05-2b141d3baffb",
   "metadata": {},
   "source": [
    "Next, the train-validation-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02660ff-c38d-4f06-83f2-2fe3b0c040c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split: 70/15/15\n",
    "X_train, X_tmp, Y_train, Y_tmp = train_test_split(X_lhs, Y, test_size=0.30, random_state=123)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_tmp, Y_tmp, test_size=0.50, random_state=123)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ab1a8-907f-4bb1-845d-37c5a4a756cf",
   "metadata": {},
   "source": [
    "Now we're applying our standard scaler again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37b8ca-39f6-4c52-828a-75816df93336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler for the inputs\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# scaler for the outputs\n",
    "y_scaler = StandardScaler().fit(Y_train)\n",
    "\n",
    "# transform train/val/test features\n",
    "Xtr = x_scaler.transform(X_train).astype(np.float32)\n",
    "Xva = x_scaler.transform(X_val).astype(np.float32)\n",
    "Xte = x_scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# transform train/val/test targets\n",
    "Ytr = y_scaler.transform(Y_train).astype(np.float32)\n",
    "Yva = y_scaler.transform(Y_val).astype(np.float32)\n",
    "Yte = y_scaler.transform(Y_test).astype(np.float32)\n",
    "\n",
    "nE = Ytr.shape[1]\n",
    "print(\"nE:\", nE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050db195-37dc-4915-8231-b3a2a38497d0",
   "metadata": {},
   "source": [
    "We have to define our DataLoader classes again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d4bd8-5f30-4676-a5ea-a44d886cf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_dataloader(Xtr, Ytr, batch_size=256, shuffle=True)\n",
    "val_loader   = make_dataloader(Xva, Yva, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26da22b-ead5-4d34-8515-2f8b8d77c979",
   "metadata": {},
   "source": [
    "### Training the One-Layer MLP\n",
    "\n",
    "Let's train our simple one-layer MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58d9d7-51df-4be7-ad03-92af4042a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our linear model\n",
    "mlp = MLP(in_dim=3, out_dim=nE, hidden=128)\n",
    "\n",
    "# set the learning rate for the optimizer\n",
    "lr = 1e-3\n",
    "\n",
    "# same optimizer as previously\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "\n",
    "# same loss function as previously\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#let's train!\n",
    "hist_mlp = train_epochs(mlp, train_loader, val_loader, epochs=3000, patience=40)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_mlp[\"train\"], label=\"train\")\n",
    "plt.plot(hist_mlp[\"val\"], label=\"val\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE (scaled log-count space)\")\n",
    "plt.title(\"Learning curves: linear surrogate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b14b7a-4cda-44cb-ad0a-d2075a3b1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat_train = mlp(torch.from_numpy(Xtr)).numpy()\n",
    "    yhat_val  = mlp(torch.from_numpy(Xva)).numpy()\n",
    "\n",
    "\n",
    "ntrain = Xtr.shape[0]\n",
    "nval = Xva.shape[0]\n",
    "\n",
    "idx_train = np.random.randint(0, ntrain)\n",
    "idx_val = np.random.randint(0, nval)\n",
    "\n",
    "# invert scaling to counts space\n",
    "yhat_train_counts = y_scaler.inverse_transform(yhat_train)[idx_train]\n",
    "ytrain_counts     = np.exp(Y_train[idx_train])\n",
    "\n",
    "yhat_val_counts = y_scaler.inverse_transform(yhat_val)[idx_val]\n",
    "yval_counts     = np.exp(Y_val[idx_val])\n",
    "\n",
    "plot_example(energy, ytrain_counts, np.exp(yhat_train_counts), title=\"Training example: target vs prediction\")\n",
    "plot_example(energy, yval_counts,   np.exp(yhat_val_counts),   title=\"Validation example: target vs prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0c5f7-bb12-40b5-86d2-bec4a749f204",
   "metadata": {},
   "source": [
    "### Adding More Layers\n",
    "\n",
    "So, that's a bit harder for the model to learn, because of the discontinuity in the data. Let's try to make the model more complex. We're going to simply add more hidden layers, and we'll implement in a way so that you can decide afterwards how many layers you want, and how many nodes each of them should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15c835-1d37-4088-9e56-bbc68cb56c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamically constructed multi-layer perceptron.\n",
    "\n",
    "    Architecture:\n",
    "        Linear -> ReLU -> Linear -> ReLU -> ... -> Linear (output)\n",
    "    i.e. ReLU after every hidden layer, no activation on the output layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_dim : int\n",
    "        Input feature dimension.\n",
    "    out_dim : int\n",
    "        Output dimension.\n",
    "    hidden_dims : Sequence[int]\n",
    "        Hidden layer widths, e.g. (64, 64, 32).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        # total dimensions includes input and output dimensions\n",
    "        dims = [in_dim, *hidden_dims, out_dim]\n",
    "\n",
    "        # generate list of layers\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            # ReLU after all layers except the last (output) layer\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf850b9-2189-4aa7-a149-ef96242491fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 3\n",
    "out_dim = Y_train.shape[1]\n",
    "\n",
    "# PICK A NUMBER OF HIDDEN LAYERS, AND A NUMBER OF \n",
    "# NODES PER LAYER\n",
    "# EXAMPLE:\n",
    "# hidden_layers = [64, 128, 256, 64]\n",
    "# produces 4 hidden layers, with the firs 64 nodes, \n",
    "# the second 128 nodes, and so on\n",
    "hidden_layers = [] # ADD NUMBER OF LAYERS AND NODES IN THIS LIST\n",
    "\n",
    "mlp = MLP(in_dim, out_dim, hidden_layers)\n",
    "\n",
    "# set the learning rate for the optimizer\n",
    "lr = 1e-4\n",
    "\n",
    "# same optimizer as previously\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "\n",
    "# same loss function as previously\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#let's train!\n",
    "hist_mlp = train_epochs(mlp, train_loader, val_loader, epochs=3000, patience=40)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_mlp[\"train\"], label=\"train\")\n",
    "plt.plot(hist_mlp[\"val\"], label=\"val\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE (scaled log-count space)\")\n",
    "plt.title(\"Learning curves: linear surrogate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9d02f-631c-409c-a17e-35def64ab246",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat_train = mlp(torch.from_numpy(Xtr)).numpy()\n",
    "    yhat_val  = mlp(torch.from_numpy(Xva)).numpy()\n",
    "\n",
    "\n",
    "ntrain = Xtr.shape[0]\n",
    "nval = Xva.shape[0]\n",
    "\n",
    "idx_train = np.random.randint(0, ntrain)\n",
    "idx_val = np.random.randint(0, nval)\n",
    "\n",
    "# invert scaling to counts space\n",
    "yhat_train_counts = y_scaler.inverse_transform(yhat_train)[idx_train]\n",
    "ytrain_counts     = np.exp(Y_train[idx_train])\n",
    "\n",
    "yhat_val_counts = y_scaler.inverse_transform(yhat_val)[idx_val]\n",
    "yval_counts     = np.exp(Y_val[idx_val])\n",
    "\n",
    "plot_example(energy, ytrain_counts, np.exp(yhat_train_counts), title=\"Training example: target vs prediction\")\n",
    "plot_example(energy, yval_counts,   np.exp(yhat_val_counts),   title=\"Validation example: target vs prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e70ab-ec63-4ea4-ae8b-8af66a4d9d93",
   "metadata": {},
   "source": [
    "At this point, you can experiment with\n",
    "* the number of hidden layers\n",
    "* the number of nodes per layer\n",
    "* the activation function (see [here](https://docs.pytorch.org/docs/stable/nn.html))\n",
    "* the learning rate and learning rate schedulers\n",
    "* the batch size\n",
    "* implement regularization methods like dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db806c52",
   "metadata": {},
   "source": [
    "### How complexity and data size interact\n",
    "\n",
    "Two knobs control generalization:\n",
    "- **data size** (more examples reduces variance)\n",
    "- **model capacity** (more expressive models reduce bias but can increase variance)\n",
    "\n",
    "We'll run a small experiment:\n",
    "- train the *same* MLP architecture on increasing numbers of training examples\n",
    "- record validation error\n",
    "\n",
    "This is a **learning curve**: it tells you whether more data is likely to help.\n",
    "\n",
    "Let's simulate a lot more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086f299-e1f3-4814-92e6-d0b510f7a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some simulated data \n",
    "X_lhs, Y_lhs = simulate_bpl_dataset_lhc(100000, energy)\n",
    "\n",
    "# transform targets to log flux\n",
    "Y = transform_targets(Y_lhs).astype(np.float32)\n",
    "\n",
    "# Train/val/test split: 90/5/5\n",
    "X_train, X_tmp, Y_train, Y_tmp = train_test_split(X_lhs, Y, test_size=0.1, random_state=123)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_tmp, Y_tmp, test_size=0.50, random_state=123)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "# scaler for the inputs\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# scaler for the outputs\n",
    "y_scaler = StandardScaler().fit(Y_train)\n",
    "\n",
    "# transform train/val/test features\n",
    "Xtr = x_scaler.transform(X_train).astype(np.float32)\n",
    "Xva = x_scaler.transform(X_val).astype(np.float32)\n",
    "Xte = x_scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# transform train/val/test targets\n",
    "Ytr = y_scaler.transform(Y_train).astype(np.float32)\n",
    "Yva = y_scaler.transform(Y_val).astype(np.float32)\n",
    "Yte = y_scaler.transform(Y_test).astype(np.float32)\n",
    "\n",
    "nE = Ytr.shape[1]\n",
    "print(\"nE:\", nE)\n",
    "\n",
    "train_loader = make_dataloader(Xtr, Ytr, batch_size=256, shuffle=True)\n",
    "val_loader   = make_dataloader(Xva, Yva, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32aa06-43e4-4c4f-9657-1b0c21c20222",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax1.scatter(X_lhs[:,0], X_lhs[:,1], s=2, alpha=0.1)\n",
    "\n",
    "for i in range(3):\n",
    "    plot_spectrum(energy, Y_lhs[i], ax=ax2,\n",
    "                  title=f\"Example spectrum {i}  (logA={X_lhs[i,0]:.2f}, Γ={X_lhs[i,1]:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d802afb-5a03-4fbc-9677-926504a4a007",
   "metadata": {},
   "source": [
    "Let's loop over different numbers of training examples and see how the performance changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07829ee0-1805-4bb1-b1b3-8baa72d580ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_train_all = [10, 100, 1000, 10000, 90000]\n",
    "\n",
    "hist_train_all, hist_val_all = [], []\n",
    "for n_train in n_train_all:\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "    print(f\"Training with {n_train} training samples\")\n",
    "\n",
    "    # sample subset from the existing training set\n",
    "    idx = rng.choice(len(Xtr), size=n_train, replace=False)\n",
    "    Xn = Xtr[idx]; Yn = Ytr[idx]\n",
    "    \n",
    "    small_train_loader = make_dataloader(Xn, Yn, batch_size=256, shuffle=True)\n",
    "    \n",
    "    in_dim = 3\n",
    "    out_dim = Y_train.shape[1]\n",
    "    \n",
    "    # six hidden layers!!!\n",
    "    hidden_layers = [64, 64, 64, 64]\n",
    "    \n",
    "    mlp = MLP(in_dim, out_dim, hidden_layers)\n",
    "    \n",
    "    # set the learning rate for the optimizer\n",
    "    lr = 1e-4\n",
    "    \n",
    "    # same optimizer as previously\n",
    "    opt = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "    \n",
    "    # same loss function as previously\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    #let's train!\n",
    "    hist_mlp = train_epochs(mlp, small_train_loader, val_loader, epochs=3000, patience=40)\n",
    "\n",
    "    hist_val_all.append(np.min(hist_mlp[\"val\"]))\n",
    "    hist_train_all.append(np.min(hist_mlp[\"train\"]))\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(hist_mlp[\"train\"], label=\"train\")\n",
    "    plt.plot(hist_mlp[\"val\"], label=\"val\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"MSE (scaled log-count space)\")\n",
    "    plt.title(f\"Learning curves: MLP surrogate, n={n_train} training examples\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c8ab8-e4ee-4cfa-9cfa-62da30b3e8b5",
   "metadata": {},
   "source": [
    "One thing you might notice is that for few training examples, the training and validation scores scissor apart very quickly, and while the performance on the training data gets better and better, this is not true for the validation data. This is typical of **overfitting**: we have a very flexible model, and few training examples, so the model massively overfits on the few training examples it has, which means it generalizes really badly to new, unseen examples. \n",
    "\n",
    "As we add more training examples, training and validation performance become more and more similar. This could be a sign that adding more complexity to the model may be helpful in allowing the model to learn more intricate features.\n",
    "\n",
    "Let's plot the best validation score as a function of training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f023ba-6532-46d0-99bb-77f45a995371",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "\n",
    "ax.plot(n_train_all, hist_train_all, marker=\"o\", lw=1, label=\"best training loss\")\n",
    "ax.plot(n_train_all, hist_val_all, marker=\"o\", lw=1, label=\"best validation loss\")\n",
    "\n",
    "ax.set_xlabel(\"Number of training examples\")\n",
    "ax.set_ylabel(\"Best MSE loss\")\n",
    "ax.set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71641836-c451-417f-8e56-a9931cd6c956",
   "metadata": {},
   "source": [
    "Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b280d-e12a-4f1f-a611-4324ef2c854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat_train = mlp(torch.from_numpy(Xtr)).numpy()\n",
    "    yhat_val  = mlp(torch.from_numpy(Xva)).numpy()\n",
    "\n",
    "\n",
    "ntrain = Xtr.shape[0]\n",
    "nval = Xva.shape[0]\n",
    "\n",
    "idx_train = np.random.randint(0, ntrain)\n",
    "idx_val = np.random.randint(0, nval)\n",
    "\n",
    "# invert scaling to counts space\n",
    "yhat_train_counts = y_scaler.inverse_transform(yhat_train)[idx_train]\n",
    "ytrain_counts     = np.exp(Y_train[idx_train])\n",
    "\n",
    "yhat_val_counts = y_scaler.inverse_transform(yhat_val)[idx_val]\n",
    "yval_counts     = np.exp(Y_val[idx_val])\n",
    "\n",
    "plot_example(energy, ytrain_counts, np.exp(yhat_train_counts), title=\"Training example: target vs prediction\")\n",
    "plot_example(energy, yval_counts,   np.exp(yhat_val_counts),   title=\"Validation example: target vs prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcb455-3f0d-4c29-86f8-de2b7cb94859",
   "metadata": {},
   "source": [
    "A useful way to visualize results is to look at the residuals as a function of input parameter. This allows you to see whether there are regions in energy where the model is doing worse, and also whether there are parts of parameter space where your model may be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2641c4-40e2-4c30-9e0c-9715e5012437",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "\n",
    "pars_train = x_scaler.inverse_transform(Xn)\n",
    "pars_val = x_scaler.inverse_transform(Xva)\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat_train = mlp(torch.from_numpy(Xtr)).numpy()\n",
    "    yhat_val  = mlp(torch.from_numpy(Xva)).numpy()\n",
    "\n",
    "\n",
    "ntrain = Xtr.shape[0]\n",
    "nval = Xva.shape[0]\n",
    "\n",
    "# invert scaling to counts space\n",
    "yhat_train_counts = np.exp(y_scaler.inverse_transform(yhat_train))\n",
    "ytrain_counts     = np.exp(Y_train)\n",
    "\n",
    "yhat_val_counts = np.exp(y_scaler.inverse_transform(yhat_val))\n",
    "yval_counts     = np.exp(Y_val)\n",
    "\n",
    "res_train = np.abs(ytrain_counts - yhat_train_counts) / ytrain_counts\n",
    "res_val = np.abs(yval_counts - yhat_val_counts) / yval_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976cac1-1bfe-45a1-8302-e074d9d53dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train_gamma1 = np.argsort(pars_train[:,0])\n",
    "idx_train_gamma2 = np.argsort(pars_train[:,1])\n",
    "idx_train_eb = np.argsort(pars_train[:,2])\n",
    "\n",
    "idx_val_gamma1 = np.argsort(pars_val[:,0])\n",
    "idx_val_gamma2 = np.argsort(pars_val[:,1])\n",
    "idx_val_eb = np.argsort(pars_val[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebca3a-2ae8-4029-9dd5-9d1ceaf1e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10,7), sharey=True, sharex=True)\n",
    "pcm1 = ax1.pcolormesh(res_train[idx_train_gamma1], vmax=0.02)\n",
    "#fig.colorbar(pcm1, ax=ax1)\n",
    "ax1.set_xlabel(\"Energy [keV]\")\n",
    "ax1.set_ylabel(r\"$\\Gamma_1$\")\n",
    "pcm2 = ax2.pcolormesh(res_train[idx_train_gamma2], vmax=0.02)\n",
    "#fig.colorbar(pcm2, ax=ax2)\n",
    "ax2.set_xlabel(\"Energy [keV]\")\n",
    "ax2.set_ylabel(r\"$\\Gamma_2$\")\n",
    "\n",
    "pcm3 = ax3.pcolormesh(res_train[idx_train_gamma2], vmax=0.02)\n",
    "fig.colorbar(pcm3, ax=ax3)\n",
    "ax3.set_xlabel(\"Energy [keV]\")\n",
    "ax3.set_ylabel(r\"$E_b$\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47311c-d3e6-4e88-b9cf-c0e3bcda17ca",
   "metadata": {},
   "source": [
    "I haven't had a chance to fix the tick labels for the axes, so they're not correct, but hopefully you get the idea! Ben has code to do this in a much nicer way! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3980a6-8708-4de3-b461-71c8e0cb8f98",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Run a grid of models with different numbers of layers and nodes per layer. Figure out what combination of nodes and layers does best.\n",
    "Use ~10000 training examples, otherwise it'll never finish. :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4906f-8793-4192-9f3f-60246598193f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad10427-6116-453a-a7da-3902082484a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c32a501c",
   "metadata": {},
   "source": [
    "## Final model and test evaluation (once)\n",
    "\n",
    "Pick the hidden size with the best validation MSE, retrain on **train+val**, and evaluate on the held-out **test** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7efea5-7d5e-4871-9da6-93741ab048e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495b83c-b108-411c-97fb-a411788b2de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b25d47-5cec-485a-b241-f7335fcc1b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04fa2272",
   "metadata": {},
   "source": [
    "Further exploring:\n",
    "* For the network above the energy grid is hardcoded into the surrogate model. This is not ideal if you want to use the surrogate model for more than one instrument. There are new neural network architectures like [Neural Operators](https://pytorch.org/blog/neuraloperatorjoins-the-pytorch-ecosystem/), [DeepONets](https://arxiv.org/abs/1910.03193) and [Neural Implicit Representations](https://medium.com/@nathaliemariehager/an-introduction-to-neural-implicit-representations-with-use-cases-ad331ca12907) that can abstract away from the grid and learn a model that predicts the output for any input (within the training data)\n",
    "* The [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) guarantees that under certain conditions, a neural network can in principle approximate any continuous function to any desired degree of accuracy. This is great! The major problem: \"in principle\" might mean, in practice, more training data than you're willing (or able) to simulate. This is where you have to be clever, and impose structure on your neural network to help it learn. What that looks like depends on the specific problem in question, but [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) maybe be a starting point, as could be the now-ubiquitous [Transformers](https://en.wikipedia.org/wiki/Transformer_(deep_learning)), or one could explore [Physics-Informed Machine Learning](https://www.nature.com/articles/s42254-021-00314-5) to directly impose physics-based constraints onto the model.\n",
    "* There are a lot of other things we haven't talked about that are now standard as part of machine learning development, such as [Dropout](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9/) and [Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization).\n",
    "* We wrote [a paper](https://ui.adsabs.harvard.edu/abs/2023arXiv231012528H/abstract) with best practices for machine learning in astronomy projects! Go and read it! \n",
    "\n",
    "\n",
    "And finally, experiment, experiment, experiment! For many machine learning problems in astronomy, there isn't a good default answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4e1b9-a74f-45c4-ae1c-d23fb7e53c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
